
ðŸš§ Problems Faced & Fixes in Next Word Predictor Project (GPT-2)

This file lists all the major issues encountered while building the next-word predictor using Hugging Face Transformers and how they were resolved.

---

ðŸ”§ Problem 1: GPT2LMHeadModel requires torch
âœ… Fix: Installed PyTorch using
    pip install torch

---

ðŸ”§ Problem 2: pad_token not found error
âœ… Fix: Set pad_token as eos_token manually:
    tokenizer.pad_token = tokenizer.eos_token

---

ðŸ”§ Problem 3: Trainer requires accelerate>=0.21.0
âœ… Fix: Installed/Updated accelerate with:
    pip install accelerate -U

---

ðŸ”§ Problem 4: fsspec issue during dataset loading
âœ… Fix: Reinstalled fsspec:
    pip uninstall fsspec -y
    pip install fsspec

---

ðŸ”§ Problem 5: Training very slow on local CPU
âœ… Fix: Reduced batch size and epochs; also suggested running on Google Colab

---

ðŸ”§ Problem 6: Tokenizer Error - Asking to pad but tokenizer has no pad token
âœ… Fix: Either set pad_token = eos_token or add:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

---

ðŸ”§ Problem 7: evaluation_strategy not accepted in TrainingArguments
âœ… Fix: Upgraded transformers to a compatible version (>=4.21) using:
    pip install transformers -U

---

ðŸ”§ Problem 8: Output not showing in generate()
âœ… Fix: Used correct tokenization, moved tensors to model.device, used model.generate()

---

ðŸ“š These were the core challenges faced during model training and inference, and resolving them helped build a solid understanding of NLP workflows and Hugging Face libraries.
